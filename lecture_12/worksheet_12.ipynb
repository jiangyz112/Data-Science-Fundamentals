{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 12\n",
    "\n",
    "Name: Yuzhe Jiang\n",
    "\n",
    "UID: U92913042\n",
    "\n",
    "Link to my repo: https://github.com/jiangyz112/Data-Science-Fundamentals/tree/worksheet_12\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Introduction to Classification\n",
    "- K Nearest Neighbors\n",
    "\n",
    "### Introduction to Classification\n",
    "\n",
    "a) For the following examples, say whether they are or aren't an example of classification.\n",
    "\n",
    "1. Predicting whether a student will be offered a job after graduating given their GPA.\n",
    "2. Predicting how long it will take (in number of months) for a student to be offered a job after graduating, given their GPA.\n",
    "3. Predicting the number of stars (1-5) a person will assign in their yelp review given the description they wrote in the review.\n",
    "4. Predicting the number of births occuring in a specified minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, This is an example of classification. The outcome is binary (offered a job or not offered a job), which fits the definition of classification.\n",
    "\n",
    "2, This is not an example of classification; it's an example of regression. The outcome is a continuous variable (number of months), which does not fit the discrete output of classification models.\n",
    "\n",
    "3, This is an example of classification. Even though the output is numeric, it represents discrete categories (1, 2, 3, 4, or 5 stars). Each category is distinct and there's no meaningful order or distance metric that applies across categories in the context of classification.\n",
    "\n",
    "4, This is not an example of classification; it's an example of regression. The outcome is a count (number of births), which is a continuous variable and better suited for regression analysis since it can take on an infinite number of possible values within a range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Given a dataset, how would you set things up such that you can both learn a model and get an idea of how this model might perform on data it has never seen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the Dataset\n",
    "Split your dataset into at least two subsets: a training set and a testing set. A common ratio is 70% of the data for training and 30% for testing, but this can vary based on the size and specifics of your dataset.\n",
    "\n",
    "\n",
    "2. Choose a Model\n",
    "Select a model that is appropriate for your problem type (e.g., classification, regression) and the nature of your data. Consider factors like the complexity of the model, the size and dimensions of your dataset, and any computational constraints.\n",
    "\n",
    "3. Train the Model\n",
    "Use the training set to train your model. This involves feeding the input features from the training set into the model and adjusting the model parameters to minimize error in predicting the training targets.\n",
    "\n",
    "4. Validate the Model (Optional)\n",
    "If you've set aside a validation set or are using cross-validation, use this step to assess how well your model generalizes to unseen data and to tune any hyperparameters. Cross-validation involves partitioning the training set into complementary subsets, training the model on one subset, and validating it on the other, iteratively.\n",
    "\n",
    "5. Test the Model\n",
    "Evaluate the model's performance on the test set. Since the test set has not been used during the training process, it serves as a proxy for new, unseen data. Use appropriate metrics to assess performance:\n",
    "\n",
    "For classification, metrics might include accuracy, precision, recall, F1 score, and ROC-AUC.\n",
    "For regression, metrics might include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "6. Interpret Results\n",
    "Analyze the results to understand how well your model might perform in real-world scenarios. If the model performs well on the test set, it suggests that it has generalized well from the training data. If performance is poor, consider revisiting your model choice, feature engineering, or training procedure.\n",
    "\n",
    "7. Iterate\n",
    "Model development is often iterative. Based on your test results, you might return to earlier steps to try different models, adjust parameters, or engineer new features to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) In your own words, briefly explain:\n",
    "\n",
    "- underfitting\n",
    "- overfitting\n",
    "\n",
    "and what signs to look out for for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple to capture the underlying structure of the data. It happens when the model doesn't have enough complexity or information to learn from the training data, leading to poor performance on both the training data and unseen data.\n",
    "\n",
    "Signs of Underfitting:\n",
    "\n",
    "The model performs poorly on the training data.\n",
    "\n",
    "The model also performs poorly on the validation or test data, showing that it is not just a matter of random chance or specific to the training set.\n",
    "\n",
    "The learning curve shows that both training and validation errors are high but close to each other.\n",
    "\n",
    "Overfitting happens when a model learns the training data too well, including its noise and outliers, rather than just the underlying pattern. As a result, it performs well on the training data but poorly on any new, unseen data because it has essentially memorized the training set rather than learned the generalizable features of the data.\n",
    "\n",
    "Signs of Overfitting:\n",
    "\n",
    "The model performs very well on the training data, often to an unrealistic degree.\n",
    "\n",
    "The performance drops significantly on the validation or test data compared to the training data.\n",
    "\n",
    "The learning curve shows a large gap between the training and validation errors, where the training error is much lower than the validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    \"Attribute A\" : [3.5, 0, 1, 2.5, 2, 1.5, 2, 3.5, 1, 3, 2, 2, 2.5, 0.5, 0., 10],\n",
    "    \"Attribute B\" : [4, 1.5, 2, 1, 3.5, 2.5, 1, 0, 3, 1.5, 4, 2, 2.5, 0.5, 2.5, 10],\n",
    "    \"Class\" : [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the data in a 2D plot coloring each scatter point one of two colors depending on its corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([x for x in 'bgrcmyk'])\n",
    "plt.scatter(..., color=colors[data[\"Class\"]].tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are points that lie far from the rest of the data. They are not necessarily invalid points however. Imagine sampling from a Normal Distribution with mean 10 and variance 1. You would expect most points you sample to be in the range [7, 13] but it's entirely possible to see 20 which, on average, should be very far from the rest of the points in the sample (unless we're VERY (un)lucky). These outliers can inhibit our ability to learn general patterns in the data since they are not representative of likely outcomes. They can still be useful in of themselves and can be analyzed in great depth depending on the problem at hand.\n",
    "\n",
    "b) Are there any points in the dataset that could be outliers? If so, please remove them from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for easier manipulation\n",
    "attribute_a = np.array(data[\"Attribute A\"])\n",
    "attribute_b = np.array(data[\"Attribute B\"])\n",
    "classes = np.array(data[\"Class\"])\n",
    "\n",
    "# Plot to visually inspect for outliers\n",
    "colors = np.array([x for x in 'bgrcmyk'])\n",
    "plt.scatter(attribute_a, attribute_b, color=colors[classes].tolist(), label=\"Original Data\")\n",
    "plt.xlabel(\"Attribute A\")\n",
    "plt.ylabel(\"Attribute B\")\n",
    "plt.title(\"Inspecting for Outliers\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Identifying outliers based on the plot\n",
    "# The point at (10, 10) is likely an outlier due to its distance from the cluster of other points\n",
    "outlier_index = np.where((attribute_a == 10) | (attribute_b == 10))\n",
    "\n",
    "# Remove the outliers from the dataset\n",
    "attribute_a_clean = np.delete(attribute_a, outlier_index)\n",
    "attribute_b_clean = np.delete(attribute_b, outlier_index)\n",
    "classes_clean = np.delete(classes, outlier_index)\n",
    "\n",
    "# Verify removal and plot cleaned data\n",
    "plt.scatter(attribute_a_clean, attribute_b_clean, color=colors[classes_clean].tolist(), label=\"Cleaned Data\")\n",
    "plt.xlabel(\"Attribute A\")\n",
    "plt.ylabel(\"Attribute B\")\n",
    "plt.title(\"Data After Removing Outliers\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Return the cleaned arrays\n",
    "(attribute_a_clean, attribute_b_clean, classes_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise points are points that could be considered invalid under the general trend in the data. These could be the result of actual errors in the data or randomness that we could attribute to oversimplification (for example if missing some information / feature about each point). Considering noise points in our model can often lead to overfitting.\n",
    "\n",
    "c) Are there any points in the dataset that could be noise points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-plotting the dataset with potential outlier removed for closer inspection\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, txt in enumerate(classes_clean):\n",
    "    plt.scatter(attribute_a_clean[i], attribute_b_clean[i], color=colors[classes_clean[i]].tolist())\n",
    "    plt.text(attribute_a_clean[i], attribute_b_clean[i], f\"{i}\", fontsize=8)\n",
    "\n",
    "plt.xlabel(\"Attribute A\")\n",
    "plt.ylabel(\"Attribute B\")\n",
    "plt.title(\"Dataset Inspection for Noise Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following point\n",
    "\n",
    "|  A  |  B  |\n",
    "|-----|-----|\n",
    "| 0.5 |  1  |\n",
    "\n",
    "d) Plot it in a different color along with the rest of the points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset again with the additional point highlighted\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Existing points\n",
    "for i, txt in enumerate(classes_clean):\n",
    "    plt.scatter(attribute_a_clean[i], attribute_b_clean[i], color=colors[classes_clean[i]].tolist())\n",
    "    plt.text(attribute_a_clean[i], attribute_b_clean[i], f\"{i}\", fontsize=8)\n",
    "\n",
    "# Additional point\n",
    "plt.scatter(0.5, 1, color='magenta', edgecolors='k', label='New Point (A=0.5, B=1)')\n",
    "\n",
    "plt.xlabel(\"Attribute A\")\n",
    "plt.ylabel(\"Attribute B\")\n",
    "plt.title(\"Dataset with New Point Highlighted\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Write a function to compute the Euclidean distance from it to all points in the dataset and pick the 3 closest points to it. In a scatter plot, draw a circle centered around the point with radius the distance of the farthest of the three points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_closest_to(example, attributes, n=3):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distances from the example point to all points in the dataset\n",
    "    and picks the n closest points to it.\n",
    "\n",
    "    Parameters:\n",
    "        example (tuple): The point of interest (A, B).\n",
    "        attributes (np.ndarray): The dataset containing all other points (Attribute A, Attribute B).\n",
    "        n (int): The number of closest points to find.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The n closest points to the example.\n",
    "        float: The distance to the farthest of the n closest points.\n",
    "    \"\"\"\n",
    "    # Compute Euclidean distances from the example to each point in the dataset\n",
    "    distances = np.sqrt(((attributes - example) ** 2).sum(axis=1))\n",
    "    \n",
    "    # Find the indices of the n smallest distances\n",
    "    closest_indices = np.argsort(distances)[:n]\n",
    "    \n",
    "    # The n closest points\n",
    "    closest_points = attributes[closest_indices]\n",
    "    \n",
    "    # Distance of the farthest of the n closest points\n",
    "    max_distance = distances[closest_indices][-1]\n",
    "    \n",
    "    return closest_points, max_distance\n",
    "\n",
    "# Example location\n",
    "location = (0.5, 1)\n",
    "\n",
    "# All points in the dataset (excluding the new point)\n",
    "attributes_clean = np.column_stack((attribute_a_clean, attribute_b_clean))\n",
    "\n",
    "# Find the 3 closest points to the example location and the radius for the circle\n",
    "closest_points, radius = n_closest_to(location, attributes_clean, n=3)\n",
    "\n",
    "# Plotting\n",
    "_, axes = plt.subplots(figsize=(8, 6))\n",
    "axes.scatter(attribute_a_clean, attribute_b_clean, color=colors[classes_clean].tolist())  # Existing dataset\n",
    "axes.scatter(location[0], location[1], color='magenta', edgecolors='k', label='New Point (A=0.5, B=1)')  # New point\n",
    "axes.scatter(closest_points[:, 0], closest_points[:, 1], color='red', edgecolors='k', label='3 Closest Points')  # 3 closest points\n",
    "cir = plt.Circle(location, radius, fill=False, alpha=0.8, linestyle='--')\n",
    "axes.add_patch(cir)\n",
    "axes.set_aspect('equal')  # Necessary so that the circle is not oval\n",
    "axes.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Write a function that takes the three points returned by your function in e) and returns the class that the majority of points have (break ties with a deterministic default class of your choosing). Print the class assigned to this new point by your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(points, classes):\n",
    "    \"\"\"\n",
    "    Determines the majority class among the given points.\n",
    "\n",
    "    Parameters:\n",
    "        points (np.ndarray): The indices of the points in the dataset.\n",
    "        classes (np.ndarray): The classes corresponding to each point in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int: The majority class among the points, with a deterministic default in case of a tie.\n",
    "    \"\"\"\n",
    "    # Extract the classes for the provided points\n",
    "    point_classes = classes[points]\n",
    "    \n",
    "    # Count the occurrences of each class\n",
    "    counts = np.bincount(point_classes)\n",
    "    \n",
    "    # Determine the majority class, default to 0 in case of a tie\n",
    "    majority_class = np.argmax(counts)\n",
    "    \n",
    "    return majority_class\n",
    "\n",
    "# Identify the indices of the 3 closest points in the original dataset\n",
    "closest_indices = [np.where((attribute_a_clean == point[0]) & (attribute_b_clean == point[1]))[0][0] for point in closest_points]\n",
    "\n",
    "# Use the majority function to determine the class of the new point\n",
    "assigned_class = majority(closest_indices, classes_clean)\n",
    "\n",
    "print(f\"The class assigned to the new point by the majority function is: {assigned_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Re-using the functions from e) and f), you should be able to assign a class to any new point. In this exercise we will implement Leave-one-out cross validiation in order to evaluate the performance of our model.\n",
    "\n",
    "For each point in the dataset:\n",
    "\n",
    "- consider that point as your test set and the rest of the data as your training set\n",
    "- classify that point using the training set\n",
    "- keep track of whether you were correct with the use of a counter\n",
    "\n",
    "Once you've iterated through the entire dataset, divide the counter by the number of points in the dataset to report an overall testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_point(example, training_attributes, training_classes, n=3):\n",
    "    \"\"\"\n",
    "    Classify an example point based on the n closest points in the training set.\n",
    "\n",
    "    Parameters:\n",
    "        example (tuple): The point to classify.\n",
    "        training_attributes (np.ndarray): Attributes of the training points.\n",
    "        training_classes (np.ndarray): Classes of the training points.\n",
    "        n (int): The number of closest points to consider.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted class for the example point.\n",
    "    \"\"\"\n",
    "    # Find the n closest points and the radius to the example in the training set\n",
    "    closest_points, _ = n_closest_to(example, training_attributes, n)\n",
    "    \n",
    "    # Identify the indices of the n closest points in the original dataset\n",
    "    closest_indices = [np.where((training_attributes[:, 0] == point[0]) & \n",
    "                                (training_attributes[:, 1] == point[1]))[0][0] for point in closest_points]\n",
    "    \n",
    "    # Determine the majority class among the closest points\n",
    "    predicted_class = majority(closest_indices, training_classes)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Initialize the counter for correct classifications\n",
    "count = 0\n",
    "\n",
    "# Loop over each point in the dataset\n",
    "for i in range(len(attribute_a_clean)):\n",
    "    # Consider the current point as the test set and the rest as the training set\n",
    "    test_point = (attribute_a_clean[i], attribute_b_clean[i])\n",
    "    actual_class = classes_clean[i]\n",
    "    \n",
    "    # Exclude the current point from the training set\n",
    "    training_attributes = np.delete(attributes_clean, i, axis=0)\n",
    "    training_classes = np.delete(classes_clean, i)\n",
    "    \n",
    "    # Classify the current point using the rest of the data as the training set\n",
    "    prediction = classify_point(test_point, training_attributes, training_classes, n=3)\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if prediction == actual_class:\n",
    "        count += 1\n",
    "\n",
    "# Compute the overall accuracy\n",
    "overall_accuracy = count / len(attribute_a_clean)\n",
    "\n",
    "print(f\"Overall accuracy = {overall_accuracy:.2f}\")\n",
    "# The overall accuracy is 0.73\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem\n",
    "\n",
    "For this question we will re-use the \"mnist_784\" dataset.\n",
    "\n",
    "a) Begin by creating a training and testing datasest from our dataset, with a 80-20 ratio, and random_state=1. You can use the `train_test_split` function from sklearn. By holding out a portion of the dataset we can evaluate how our model generalizes to unseen data (i.e. data it did not learn from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fetch the MNIST dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Split the dataset into training and testing sets with an 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For K ranging from 1 to 20:\n",
    "\n",
    "1. train a KNN on the training data\n",
    "2. record the training and testing accuracy\n",
    "\n",
    "Plot a graph of the training and testing set accuracy as a function of the number of neighbors K (on the same plot). Which value of K is optimal? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# Split the dataset into an 80-20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize lists to store accuracies\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Train and evaluate a KNN classifier for K from 1 to 20\n",
    "for K in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Record training accuracy\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    # Record testing accuracy\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 21), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, 21), test_accuracies, label='Testing Accuracy')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Accuracy vs. Number of Neighbors K')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" This script trains a KNN classifier for each K value from 1 to 20 and plots the training and testing accuracies.\n",
    "    The graph will help identify the optimal K value. The optimal K is usually the one that maximizes testing accuracy while keeping the model as simple as possible. \n",
    "    Look for the point where the testing accuracy begins to level off or decrease as K increases, indicating that adding more neighbors does not improve \n",
    "    the model's ability to generalize to unseen data. The choice of K is critical in balancing the bias-variance tradeoff inherent in the model's complexity.\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using the best model from b), pick an image at random and plot it next to its K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming optimal K and MNIST data are already defined\n",
    "K_optimal = # Your optimal K value\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train the best KNN model\n",
    "knn_best = KNeighborsClassifier(n_neighbors=K_optimal)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "# Select a random image from the test set\n",
    "random_idx = np.random.randint(X_test.shape[0])\n",
    "random_image = X_test[random_idx].reshape(1, -1)\n",
    "\n",
    "# Find the K nearest neighbors of the random test image\n",
    "distances, indices = knn_best.kneighbors(random_image, n_neighbors=K_optimal)\n",
    "\n",
    "# Plot the random test image and its K nearest neighbors\n",
    "plt.figure(figsize=(2 * (K_optimal + 1), 2))\n",
    "plt.subplot(1, K_optimal + 1, 1)\n",
    "plt.imshow(random_image.reshape(28, 28), cmap='gray')\n",
    "plt.title('Test Image')\n",
    "plt.axis('off')\n",
    "\n",
    "for i, neighbor_idx in enumerate(indices[0], start=2):\n",
    "    plt.subplot(1, K_optimal + 1, i)\n",
    "    plt.imshow(X_train[neighbor_idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Neighbor {i-1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Using a dimensionality reduction technique discussed in class, reduce the dimensionality of the dataset before applying a KNN model. Repeat b) and discuss similarities and differences to the previous model. Briefly discuss your choice of dimension and why you think the performance / accuracy of the model has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Lists to store accuracies\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Loop over K values\n",
    "for K in range(1, 21):\n",
    "    # Define the pipeline\n",
    "    pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    model = make_pipeline(pca, knn)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Record training and testing accuracies\n",
    "    train_accuracies.append(model.score(X_train, y_train))\n",
    "    test_accuracies.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plotting accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 21), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, 21), test_accuracies, label='Testing Accuracy')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Accuracy vs. Number of Neighbors K with PCA')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Comparing this model to the previous one without dimensionality reduction, you may observe:\n",
    "\n",
    "Different Optimal K: Dimensionality reduction can change the relationship between points, potentially altering the optimal K value.\n",
    "Improved Testing Accuracy: By reducing dimensions, PCA may help mitigate the curse of dimensionality, improving KNN's performance on the test set.\n",
    "Faster Training Times: Reduced dimensions lead to faster computations.\n",
    "\n",
    "The performance/accuracy changes because PCA tends to filter out noise and less informative features, allowing the KNN algorithm to focus on the most relevant aspects of the data.\n",
    "The choice of dimension (e.g., components covering 95% of variance) is a balance between retaining essential information and removing noise or redundant information.\n",
    "Depending on how well the reduced dimensions capture the essence of the data, you might see improved accuracy and computational efficiency.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm Prep (Part 1)\n",
    "\n",
    "Compete in the Titanic Data Science Competition on Kaggle: https://www.kaggle.com/c/titanic \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Add at least 2 new features to the dataset (explain your reasoning below)\n",
    "2. Use KNN (and only KNN) to predict survival\n",
    "3. Explain your process below and choice of K\n",
    "4. Make a submission to the competition and provide a link to your submission below.\n",
    "5. Show your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature engineering\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n",
    "test_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Preprocess data: This is a simplified example. You'd need to preprocess the data appropriately.\n",
    "# For the purpose of this example, let's focus on numerical columns and ignore categorical columns like 'Embarked', 'Sex'.\n",
    "X = train_df[['Pclass', 'Age', 'Fare', 'FamilySize', 'IsAlone']].fillna(0)\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Splitting the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN model with pipeline for scaling features\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {'knn__n_neighbors': range(1, 20)}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid.best_score_}\")\n",
    "\n",
    "# Predict on test data\n",
    "X_test = test_df[['Pclass', 'Age', 'Fare', 'FamilySize', 'IsAlone']].fillna(0)\n",
    "predictions = grid.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two features to add are:\n",
    "\n",
    "FamilySize: This combines SibSp (number of siblings/spouses aboard) and Parch (number of parents/children aboard). The reasoning is that the survival chances might be different for passengers traveling alone versus those with family.\n",
    "\n",
    "IsAlone: A binary feature indicating if the passenger is traveling alone. This is derived from FamilySize. The hypothesis is that those traveling alone might have a different survival rate than those with family.\n",
    "\n",
    "---------------\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Load the Dataset\n",
    "\n",
    "First, we need to load the training and testing datasets to understand the data and identify opportunities for feature engineering.\n",
    "\n",
    "2. Feature Engineering\n",
    "\n",
    "Add two features: FamilySize, IsAlone\n",
    "\n",
    "3. Preprocessing\n",
    "\n",
    "Preprocess the data to handle missing values, convert categorical variables to numerical, and normalize/standardize numerical features if necessary for KNN.\n",
    "\n",
    "4. Choosing K\n",
    "\n",
    "The optimal K is found using cross-validation, such as grid search with cross-validation.\n",
    "\n",
    "5. Training the Model\n",
    "\n",
    "Train the KNN model on the training data.\n",
    "\n",
    "6. Making Predictions\n",
    "\n",
    "Use the trained model to predict survival on the test dataset.\n",
    "\n",
    "7. Submission\n",
    "\n",
    "Create a submission file.\n",
    "\n",
    "-------\n",
    "Link:\n",
    "https://www.kaggle.com/competitions/titanic/submissions#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
